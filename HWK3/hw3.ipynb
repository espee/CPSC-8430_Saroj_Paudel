{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35463607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering, default_data_collator, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import collections\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06779b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spoken squad datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b091bcf487d4a70964692408c2e709a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bba16ade2840749be51c3da2cf85f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c2905bc9e6421395be545209cd52bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_WER44 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "229d90a595404d29929ead7e97b51bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_WER54 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SpokenSQuAD dataset files\n",
    "spoken_train = 'spoken_train-v1.1.json'\n",
    "spoken_test = 'spoken_test-v1.1.json'\n",
    "spoken_test_WER44 = 'spoken_test-v1.1_WER44.json'\n",
    "spoken_test_WER54 = 'spoken_test-v1.1_WER54.json'\n",
    "\n",
    "# Reformat the json data \n",
    "def reformat_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "        examples = []\n",
    "    # iterate over json 'data' list\n",
    "    for elem in json_data['data']:\n",
    "        title = elem['title']\n",
    "        # iterate over paragraphs\n",
    "        for paragraph in elem['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            # iterate over question-answers for this paragraph\n",
    "            for qa in paragraph['qas']:\n",
    "                example = {}\n",
    "                example['id'] = qa['id']\n",
    "                example['title'] = title.strip()\n",
    "                example['context'] = context.strip()\n",
    "                example['question'] = qa['question'].strip()\n",
    "                example['answers'] = {}\n",
    "                example['answers']['answer_start'] = [answer[\"answer_start\"] for answer in qa['answers']]\n",
    "                example['answers']['text'] = [answer[\"text\"] for answer in qa['answers']]\n",
    "                examples.append(example)\n",
    "    \n",
    "    out_dict = {'data': examples}\n",
    "    output_json_file = 'out_'+json_file\n",
    "    with open(output_json_file, 'w') as f:\n",
    "        json.dump(out_dict, f)\n",
    "    return output_json_file\n",
    "\n",
    "\n",
    "print(\"Loading spoken squad datasets...\")\n",
    "# reformat the json data\n",
    "spoken_train = reformat_json(spoken_train)\n",
    "spoken_test = reformat_json(spoken_test)\n",
    "spoken_test_WER44 = reformat_json(spoken_test_WER44)\n",
    "spoken_test_WER54 = reformat_json(spoken_test_WER54)\n",
    "\n",
    "spoken_squad_dataset = load_dataset('json',data_files= { 'train': spoken_train,'validation': spoken_test, \n",
    "                                                        'test_WER44': spoken_test_WER44,'test_WER54': spoken_test_WER54 }, \n",
    "                                    field = 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bc3e64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model and Tokenizer...\n",
      "DebertaV2ForQuestionAnswering(\n",
      "  (deberta): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(251000, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Use the model bert-base-uncased from huggingface.co\n",
    "model_name = \"timpal0l/mdeberta-v3-base-squad2\"\n",
    "print(\"Initializing Model and Tokenizer...\")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66c03dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20382c6a961441c68a80ea3eb5a89621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 384\n",
    "stride = 64\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    #tokenize question-context\n",
    "    questions = [question.strip() for question in examples['question']]\n",
    "    inputs = tokenizer(\n",
    "        questions, \n",
    "        examples['context'],\n",
    "        max_length = max_length,\n",
    "        truncation = 'only_second',\n",
    "        stride = stride, \n",
    "        return_overflowing_tokens = True,\n",
    "        return_offsets_mapping=True, \n",
    "        padding = 'max_length'\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop('offset_mapping')\n",
    "    sample_map = inputs.pop('overflow_to_sample_mapping')\n",
    "    answers = examples['answers']\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer['answer_start'][0]\n",
    "        end_char = answer['answer_start'][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # find start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1: \n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # if answer not fully inside context, apply label (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "    \n",
    "    inputs['start_positions'] = start_positions\n",
    "    inputs['end_positions'] = end_positions\n",
    "    return inputs\n",
    "\n",
    "\n",
    "print(\"Preprocessing the training data...\")\n",
    "train_dataset = spoken_squad_dataset['train'].map(preprocess_training_examples,batched = True,remove_columns=spoken_squad_dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580329f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to preprocess validation/test examples (performs tokenization, windowing)\n",
    "def process_validation_examples(examples):\n",
    "    questions = [question.strip() for question in examples['question']]\n",
    "    inputs = tokenizer(\n",
    "        questions, \n",
    "        examples['context'],\n",
    "        max_length = max_length,\n",
    "        truncation = 'only_second',\n",
    "        stride = stride, \n",
    "        return_overflowing_tokens = True,\n",
    "        return_offsets_mapping=True, \n",
    "        padding = 'max_length'\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop('overflow_to_sample_mapping')\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs['input_ids'])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offsets = inputs['offset_mapping'][i]\n",
    "        inputs[\"offset_mapping\"][i] = [offset if sequence_ids[k] == 1 else None for k, offset in enumerate(offsets)]\n",
    "\n",
    "    inputs['example_id'] = example_ids\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403ffb2b-55a6-4e99-9ab0-5a662078ebee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test data (NO NOISE: 22.73% WER)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f9dd787bd84fffa19718f4bbb90f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing V1 noise test data (44.22% WER)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9881835e737c49dab81ec2e02afaf22b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing V2 noise test data (54.82% WER)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1c0213e5654d5cb51be518239a33a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Preprocess the three evaluation data sets\n",
    "\n",
    "print(\"Preprocessing test data (NO NOISE: 22.73% WER)...\")\n",
    "validation_dataset = spoken_squad_dataset['validation'].map(process_validation_examples,batched = True,remove_columns=spoken_squad_dataset['validation'].column_names)\n",
    "print(\"Preprocessing V1 noise test data (44.22% WER)...\")\n",
    "test_WER44_dataset = spoken_squad_dataset['test_WER44'].map(process_validation_examples,batched = True,remove_columns=spoken_squad_dataset['test_WER44'].column_names)\n",
    "print(\"Preprocessing V2 noise test data (54.82% WER)...\")\n",
    "test_WER54_dataset = spoken_squad_dataset['test_WER54'].map(process_validation_examples,batched = True,remove_columns=spoken_squad_dataset['test_WER54'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a505c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define a compute_metric() function \n",
    "metric = evaluate.load(\"squad\")\n",
    "n_best = 10\n",
    "max_answer_length = 32\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    #create default item if not present in the dictionary \n",
    "    example_to_features = collections.defaultdict(list)  \n",
    "    for idx, feature in enumerate(features): \n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "    \n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        \n",
    "        # loop thru all features associated with example ID\n",
    "        for feature_index in example_to_features[example_id]: \n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "            \n",
    "            start_indices = np.argsort(start_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            end_indices = np.argsort(end_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            for start_index in start_indices: \n",
    "                for end_index in end_indices: \n",
    "                    # skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None: \n",
    "                        continue\n",
    "                    # skip answers with a length that is either <0 or >max_answer_length\n",
    "                    if end_index < start_index or end_index-start_index+1 > max_answer_length: \n",
    "                        continue\n",
    "                    answer = {\"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                              \"logit_score\": start_logit[start_index] + end_logit[end_index] }\n",
    "                    answers.append(answer)\n",
    "                    \n",
    "        # select answer with best score among n_best based on logit score\n",
    "        if len(answers) > 0: \n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else: \n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "        \n",
    "    reference_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    \n",
    "    #return 'exact matches' and 'f1' score\n",
    "    return metric.compute(predictions=predicted_answers, references=reference_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3915a992",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataloader for all datasets...\n",
      "Dataloader creatred...\n"
     ]
    }
   ],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "validation_set.set_format(\"torch\")\n",
    "test_WER44_set = test_WER44_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "test_WER44_set.set_format(\"torch\")\n",
    "test_WER54_set = test_WER54_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "test_WER54_set.set_format(\"torch\")\n",
    "\n",
    "print(\"Creating dataloader for all datasets...\")\n",
    "train_dataloader = DataLoader(train_dataset, shuffle = True, collate_fn=default_data_collator, batch_size=8)\n",
    "eval_dataloader = DataLoader(validation_set, collate_fn=default_data_collator, batch_size=8)\n",
    "test_WER44_dataloader = DataLoader(test_WER44_set, collate_fn=default_data_collator, batch_size=8)\n",
    "test_WER54_dataloader = DataLoader(test_WER54_set, collate_fn=default_data_collator, batch_size=8)\n",
    "print(\"Dataloader creatred...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b290a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Function to evaluate the model accuracy on a given dataset \n",
    "def evaluate_model(model, dataloader, dataset, dataset_before_preprocessing, accelerator=None):\n",
    "    #Use Accelerator with 16bit floating point\n",
    "    if not accelerator: \n",
    "        accelerator = Accelerator(mixed_precision='fp16')\n",
    "        model, dataloader = accelerator.prepare(model, dataloader)\n",
    "    \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        with torch.no_grad(): \n",
    "            outputs = model(**batch)\n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(dataset)]\n",
    "    end_logits = end_logits[: len(dataset)]\n",
    "\n",
    "    metrics = compute_metrics(start_logits, end_logits, dataset, dataset_before_preprocessing)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca2ea22a-9cb2-4c50-bdfa-d23ace25cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training the model\n",
    "def train_model(model=model, train_dataloader=train_dataloader, eval_dataloader=eval_dataloader, epochs = 1):\n",
    "    training_steps = epochs * len(train_dataloader)\n",
    "\n",
    "    accelerator = Accelerator(mixed_precision='fp16')\n",
    "    optimizer = AdamW(model.parameters(), lr = 2e-5)\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
    "    \n",
    "    #Using scheduler to ramp down the learning rate linearly\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0,num_training_steps=training_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # evaluate after each epoch \n",
    "        accelerator.print(\"Evaluation...\")\n",
    "        metrics = evaluate_model(model, eval_dataloader, validation_dataset, spoken_squad_dataset['validation'], accelerator)\n",
    "        print(f\"epoch {epoch}:\", metrics)\n",
    "    \n",
    "    # save the trained model\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\"./\", save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac592eaa-468d-4183-8d86-98fd028747f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8d3d78fd704577a0577388a6ff1ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/699 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f15c5a17a907499cb49942910c10f348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0: {'exact_match': 70.64100168192861, 'f1': 79.85560427742683}\n"
     ]
    }
   ],
   "source": [
    "### UNCOMMENT TO TRAIN THE MODEL\n",
    "#print(\"Fine-tuning the model...\")\n",
    "#train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bee624d7-5293-419a-bb0b-b6bbd7adf9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DebertaV2ForQuestionAnswering(\n",
      "  (deberta): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(251000, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "Evaluating model on Validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb211392973e47069c2597e501fab3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/699 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d84d61e8714394b812b9042364e85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on V1 Noise dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30bc28f4bc9b4b66b3bb029581ee6879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c983893659f49919bfb54b565968b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on V2 Noise dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96ed2aa24d814a4cb8951d627640b73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/700 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65455629e8c64b699cf50acffa3bc0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= RESULTS =============\n",
      "Validation dataset    (NO NOISE - WER = 22.73%) - exact match: 70.64100168192861, F1 score: 79.85560427742683\n",
      "V1 Noise dataset (WER = 44.22%) - exact match: 45.000934404784154, F1 score: 60.410109979058916\n",
      "V2 Noise dataset (WER = 54.82%) - exact match: 31.15305550364418, F1 score: 46.11408550942854\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "#Load Finetuned model\n",
    "#finetuned_model_name = \"timpal0l/mdeberta-v3-base-squad2\"    #original model\n",
    "finetuned_model_name = \"./\"\n",
    "finetuned_model = AutoModelForQuestionAnswering.from_pretrained(finetuned_model_name)\n",
    "#print(finetuned_model)\n",
    "\n",
    "### EVALUATE FINETUNED MODEL for validation dataset test data set with noises\n",
    "print(\"Evaluating model on Validation dataset...\")\n",
    "test_metrics = evaluate_model(finetuned_model, eval_dataloader, validation_dataset, spoken_squad_dataset['validation'])\n",
    "print(\"Evaluating model on V1 Noise dataset...\")\n",
    "test_v1_metrics = evaluate_model(finetuned_model, test_WER44_dataloader, test_WER44_dataset, spoken_squad_dataset['test_WER44'])\n",
    "print(\"Evaluating model on V2 Noise dataset...\")\n",
    "test_v2_metrics = evaluate_model(finetuned_model, test_WER54_dataloader, test_WER54_dataset, spoken_squad_dataset['test_WER54'])\n",
    "\n",
    "print(\"============= RESULTS =============\")\n",
    "print(\"Validation dataset    (NO NOISE - WER = 22.73%) - exact match: \" + str(test_metrics['exact_match']) + \", F1 score: \" + str(test_metrics['f1']))\n",
    "print(\"V1 Noise dataset (WER = 44.22%) - exact match: \" + str(test_v1_metrics['exact_match']) + \", F1 score: \" + str(test_v1_metrics['f1']))\n",
    "print(\"V2 Noise dataset (WER = 54.82%) - exact match: \" + str(test_v2_metrics['exact_match']) + \", F1 score: \" + str(test_v2_metrics['f1']))\n",
    "print(\"===================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba3d6ea-c49b-41ab-a314-8752c7306c4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
