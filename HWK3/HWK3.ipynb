{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35463607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering, default_data_collator, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import collections\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06779b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spoken squad datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4ce93ca10241228b84b8ac647e499e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02ff19679b2b474aac26322d386cf6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b801b7608d4042a87d496c07bfe28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_WER44 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54304eec4f7840e78af7f2ea7cb71c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_WER54 split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SpokenSQuAD dataset files\n",
    "spoken_train = 'spoken_train-v1.1.json'\n",
    "spoken_test = 'spoken_test-v1.1.json'\n",
    "spoken_test_WER44 = 'spoken_test-v1.1_WER44.json'\n",
    "spoken_test_WER54 = 'spoken_test-v1.1_WER54.json'\n",
    "\n",
    "# Reformat the json data \n",
    "def reformat_json(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "        examples = []\n",
    "    # iterate over json 'data' list\n",
    "    for elem in json_data['data']:\n",
    "        title = elem['title']\n",
    "        # iterate over paragraphs\n",
    "        for paragraph in elem['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            # iterate over question-answers for this paragraph\n",
    "            for qa in paragraph['qas']:\n",
    "                example = {}\n",
    "                example['id'] = qa['id']\n",
    "                example['title'] = title.strip()\n",
    "                example['context'] = context.strip()\n",
    "                example['question'] = qa['question'].strip()\n",
    "                example['answers'] = {}\n",
    "                example['answers']['answer_start'] = [answer[\"answer_start\"] for answer in qa['answers']]\n",
    "                example['answers']['text'] = [answer[\"text\"] for answer in qa['answers']]\n",
    "                examples.append(example)\n",
    "    \n",
    "    out_dict = {'data': examples}\n",
    "    output_json_file = 'out_'+json_file\n",
    "    with open(output_json_file, 'w') as f:\n",
    "        json.dump(out_dict, f)\n",
    "    return output_json_file\n",
    "\n",
    "\n",
    "print(\"Loading spoken squad datasets...\")\n",
    "# reformat the json data\n",
    "spoken_train = reformat_json(spoken_train)\n",
    "spoken_test = reformat_json(spoken_test)\n",
    "spoken_test_WER44 = reformat_json(spoken_test_WER44)\n",
    "spoken_test_WER54 = reformat_json(spoken_test_WER54)\n",
    "\n",
    "spoken_squad_dataset = load_dataset('json',data_files= { 'train': spoken_train,'validation': spoken_test, \n",
    "                                                        'test_WER44': spoken_test_WER44,'test_WER54': spoken_test_WER54 }, \n",
    "                                    field = 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bc3e64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Model and Tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Use the model bert-base-uncased from huggingface.co\n",
    "model_name = \"bert-base-uncased\"\n",
    "print(\"Initializing Model and Tokenizer...\")\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66c03dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing training data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737da5d4d26245dab76b47cff0f39798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37111 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 384\n",
    "stride = 64\n",
    "\n",
    "\"\"\" \n",
    "Preprocessing the training examples. \n",
    "(1)tokenize examples into question-context token sequences of the form: [CLS] question [SEP] context [SEP] ....       \n",
    "(2)apply windowing with given stride\n",
    "(3)compute output labels (start_index, end_index)\n",
    "(4)if answer not fully within windowed context, set label to (0, 0)\n",
    "\"\"\"\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    #tokenize question-context\n",
    "    questions = [question.strip() for question in examples['question']]\n",
    "    inputs = tokenizer(\n",
    "        questions, \n",
    "        examples['context'],\n",
    "        max_length = max_length,\n",
    "        truncation = 'only_second',\n",
    "        stride = stride, \n",
    "        return_overflowing_tokens = True,\n",
    "        return_offsets_mapping=True, \n",
    "        padding = 'max_length'\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop('offset_mapping')\n",
    "    sample_map = inputs.pop('overflow_to_sample_mapping')\n",
    "    answers = examples['answers']\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        start_char = answer['answer_start'][0]\n",
    "        end_char = answer['answer_start'][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # find start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1: \n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # if answer not fully inside context, label is (0, 0)\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "    \n",
    "    inputs['start_positions'] = start_positions\n",
    "    inputs['end_positions'] = end_positions\n",
    "    return inputs\n",
    "\n",
    "\n",
    "print(\"Preprocessing training data...\")\n",
    "train_dataset = spoken_squad_dataset['train'].map(\n",
    "    preprocess_training_examples,\n",
    "    batched = True,\n",
    "    remove_columns=spoken_squad_dataset['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "580329f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to preprocess validation/test examples (performs tokenization, windowing)\n",
    "def process_validation_examples(examples):\n",
    "    questions = [question.strip() for question in examples['question']]\n",
    "    inputs = tokenizer(\n",
    "        questions, \n",
    "        examples['context'],\n",
    "        max_length = max_length,\n",
    "        truncation = 'only_second',\n",
    "        stride = stride, \n",
    "        return_overflowing_tokens = True,\n",
    "        return_offsets_mapping=True, \n",
    "        padding = 'max_length'\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop('overflow_to_sample_mapping')\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs['input_ids'])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offsets = inputs['offset_mapping'][i]\n",
    "        inputs[\"offset_mapping\"][i] = [offset if sequence_ids[k] == 1 else None for k, offset in enumerate(offsets)]\n",
    "\n",
    "    inputs['example_id'] = example_ids\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403ffb2b-55a6-4e99-9ab0-5a662078ebee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing test data (NO NOISE: 22.73% WER)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72ca24f947447dcbd5f303e6dc2b7a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing V1 noise test data (44.22% WER)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e49f4694084e58869c4f107a6d5829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing V2 noise test data (54.82% WER)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e36137948c5448aade74f7abb43c380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5351 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Preprocess the three evaluation data sets\n",
    "\n",
    "print(\"Preprocessing test data (NO NOISE: 22.73% WER)...\")\n",
    "validation_dataset = spoken_squad_dataset['validation'].map(\n",
    "    process_validation_examples,\n",
    "    batched = True,\n",
    "    remove_columns=spoken_squad_dataset['validation'].column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing V1 noise test data (44.22% WER)...\")\n",
    "test_WER44_dataset = spoken_squad_dataset['test_WER44'].map(\n",
    "    process_validation_examples,\n",
    "    batched = True,\n",
    "    remove_columns=spoken_squad_dataset['test_WER44'].column_names\n",
    ")\n",
    "\n",
    "print(\"Preprocessing V2 noise test data (54.82% WER)...\")\n",
    "test_WER54_dataset = spoken_squad_dataset['test_WER54'].map(\n",
    "    process_validation_examples,\n",
    "    batched = True,\n",
    "    remove_columns=spoken_squad_dataset['test_WER54'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a505c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Define a compute_metric() function \n",
    "metric = evaluate.load(\"squad\")\n",
    "n_best = 10\n",
    "max_answer_length = 32\n",
    "\n",
    "def compute_metrics(start_logits, end_logits, features, examples):\n",
    "    #create default item if not present in the dictionary \n",
    "    example_to_features = collections.defaultdict(list)  \n",
    "    for idx, feature in enumerate(features): \n",
    "        example_to_features[feature[\"example_id\"]].append(idx)\n",
    "    \n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        \n",
    "        # loop thru all features associated with example ID\n",
    "        for feature_index in example_to_features[example_id]: \n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = features[feature_index][\"offset_mapping\"]\n",
    "            \n",
    "            start_indices = np.argsort(start_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            end_indices = np.argsort(end_logit)[-1: -n_best - 1: -1].tolist()\n",
    "            for start_index in start_indices: \n",
    "                for end_index in end_indices: \n",
    "                    # skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None: \n",
    "                        continue\n",
    "                    # skip answers with a length that is either <0 or >max_answer_length\n",
    "                    if end_index < start_index or end_index-start_index+1 > max_answer_length: \n",
    "                        continue\n",
    "                    answer = {\"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                              \"logit_score\": start_logit[start_index] + end_logit[end_index] }\n",
    "                    answers.append(answer)\n",
    "                    \n",
    "        # select answer with best score among n_best based on logit score\n",
    "        if len(answers) > 0: \n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else: \n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "        \n",
    "    reference_answers = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
    "    \n",
    "    #return 'exact matches' and 'f1' score\n",
    "    return metric.compute(predictions=predicted_answers, references=reference_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3915a992",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataloader for all datasets...\n",
      "Dataloader creatred...\n"
     ]
    }
   ],
   "source": [
    "train_dataset.set_format(\"torch\")\n",
    "validation_set = validation_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "validation_set.set_format(\"torch\")\n",
    "test_WER44_set = test_WER44_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "test_WER44_set.set_format(\"torch\")\n",
    "test_WER54_set = test_WER54_dataset.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "test_WER54_set.set_format(\"torch\")\n",
    "\n",
    "print(\"Creating dataloader for all datasets...\")\n",
    "train_dataloader = DataLoader(train_dataset, shuffle = True, collate_fn=default_data_collator, batch_size=8)\n",
    "eval_dataloader = DataLoader(validation_set, collate_fn=default_data_collator, batch_size=8)\n",
    "test_WER44_dataloader = DataLoader(test_WER44_set, collate_fn=default_data_collator, batch_size=8)\n",
    "test_WER54_dataloader = DataLoader(test_WER54_set, collate_fn=default_data_collator, batch_size=8)\n",
    "print(\"Dataloader creatred...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b290a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Function to evaluate the model accuracy on a given dataset \n",
    "def evaluate_model(model, dataloader, dataset, dataset_before_preprocessing, accelerator=None):\n",
    "    #Use Accelerator with 16bit floating point\n",
    "    if not accelerator: \n",
    "        accelerator = Accelerator(mixed_precision='fp16')\n",
    "        model, dataloader = accelerator.prepare(model, dataloader)\n",
    "    \n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for batch in tqdm(dataloader):\n",
    "        with torch.no_grad(): \n",
    "            outputs = model(**batch)\n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(dataset)]\n",
    "    end_logits = end_logits[: len(dataset)]\n",
    "\n",
    "    metrics = compute_metrics(start_logits, end_logits, dataset, dataset_before_preprocessing)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca2ea22a-9cb2-4c50-bdfa-d23ace25cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training the model\n",
    "def train_model(model=model, train_dataloader=train_dataloader, eval_dataloader=eval_dataloader, epochs = 1):\n",
    "    training_steps = epochs * len(train_dataloader)\n",
    "\n",
    "    #accelerator = Accelerator(mixed_precision='fp16')\n",
    "    optimizer = AdamW(model.parameters(), lr = 2e-5)\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(model, optimizer, train_dataloader, eval_dataloader)\n",
    "    \n",
    "    #Using scheduler to ramp down the learning rate linearly\n",
    "    lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0,num_training_steps=training_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # evaluate after each epoch \n",
    "        accelerator.print(\"Evaluation...\")\n",
    "        metrics = evaluate_model(model, eval_dataloader, validation_dataset, spoken_squad_dataset['validation'], accelerator)\n",
    "        print(f\"epoch {epoch}:\", metrics)\n",
    "    \n",
    "    # save the trained model\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\"./\", save_function=accelerator.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac592eaa-468d-4183-8d86-98fd028747f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNCOMMENT TO TRAIN THE MODEL\n",
    "#print(\"Fine-tuning the model...\")\n",
    "#train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bee624d7-5293-419a-bb0b-b6bbd7adf9f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForQuestionAnswering(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "Evaluating model on Validation dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8d6b3ced81446693f4fb0caaab8359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/678 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e150d7d13a9f4e10ba5fb20101495f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on V1 Noise dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13509dbf447648ddaeee9e43225b6e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/679 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0641f97dd58f4da9ab7cf1f032796ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on V2 Noise dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1612a5f297f34a2888329c65f7437caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/679 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc1b2ec1d57495a838194e59762bfdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5351 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= RESULTS =============\n",
      "Validation dataset    (NO NOISE - WER = 22.73%) - exact match: 61.166137170622314, F1 score: 71.89843641916406\n",
      "V1 Noise dataset (WER = 44.22%) - exact match: 38.74042235096244, F1 score: 53.667193882586005\n",
      "V2 Noise dataset (WER = 54.82%) - exact match: 26.966922070641, F1 score: 40.71879797236333\n",
      "===================================\n"
     ]
    }
   ],
   "source": [
    "#Load Finetuned model\n",
    "finetuned_model_name = \"./\"\n",
    "finetuned_model = AutoModelForQuestionAnswering.from_pretrained(finetuned_model_name)\n",
    "print(finetuned_model)\n",
    "\n",
    "### EVALUATE FINETUNED MODEL for validation dataset test data set with noises\n",
    "print(\"Evaluating model on Validation dataset...\")\n",
    "test_metrics = evaluate_model(finetuned_model, eval_dataloader, validation_dataset, spoken_squad_dataset['validation'])\n",
    "print(\"Evaluating model on V1 Noise dataset...\")\n",
    "test_v1_metrics = evaluate_model(finetuned_model, test_WER44_dataloader, test_WER44_dataset, spoken_squad_dataset['test_WER44'])\n",
    "print(\"Evaluating model on V2 Noise dataset...\")\n",
    "test_v2_metrics = evaluate_model(finetuned_model, test_WER54_dataloader, test_WER54_dataset, spoken_squad_dataset['test_WER54'])\n",
    "\n",
    "print(\"============= RESULTS =============\")\n",
    "print(\"Validation dataset    (NO NOISE - WER = 22.73%) - exact match: \" + str(test_metrics['exact_match']) + \", F1 score: \" + str(test_metrics['f1']))\n",
    "print(\"V1 Noise dataset (WER = 44.22%) - exact match: \" + str(test_v1_metrics['exact_match']) + \", F1 score: \" + str(test_v1_metrics['f1']))\n",
    "print(\"V2 Noise dataset (WER = 54.82%) - exact match: \" + str(test_v2_metrics['exact_match']) + \", F1 score: \" + str(test_v2_metrics['f1']))\n",
    "print(\"===================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ddd177-b697-457b-9dc2-44175530bd47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
